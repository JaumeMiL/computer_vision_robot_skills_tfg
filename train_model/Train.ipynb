{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenament EUREKA\n",
    "\n",
    "Aquest notebook entrena el model EUREKA utilitzant els fitxers d'anotaci√≥ `.txt` (`Annot_TrainList.txt`, `classIdx.txt`) i els v√≠deos `.avi` locals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Acceleraci√≥ Apple Silicon detectada: Utilitzant MPS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. CONFIGURACI√ì MAC (MPS) ---\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"Acceleraci√≥ Apple Silicon detectada: Utilitzant MPS\")\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"No s'ha detectat GPU Metal. Utilitzant CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. CONFIGURACI√ì I RUTES ---\n",
    "M_KEY_FRAMES = 15\n",
    "NUM_LANDMARKS = 21\n",
    "COMPONENTS = 2\n",
    "INPUT_SIZE = (NUM_LANDMARKS ** 2) * COMPONENTS * (M_KEY_FRAMES - 1)\n",
    "\n",
    "# Canvia aix√≤ si tens els fitxers en un altre lloc\n",
    "PATH_VIDEOS_ROOT = \"videos\"\n",
    "PATH_ANNOTATIONS_DIR = \"annotations\"\n",
    "\n",
    "FILE_TRAIN_LIST = os.path.join(PATH_ANNOTATIONS_DIR, \"Annot_TrainList.txt\")\n",
    "FILE_CLASS_IDX = os.path.join(PATH_ANNOTATIONS_DIR, \"classIdx.txt\")\n",
    "\n",
    "# Verificaci√≥\n",
    "if not os.path.exists(FILE_TRAIN_LIST):\n",
    "    print(f\"ERROR: No trobo {FILE_TRAIN_LIST}. Crea la carpeta 'annotations' i posa-hi els fitxers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Llegint classes de: annotations/classIdx.txt\n",
      "   ‚ÑπÔ∏è Primeres l√≠nies del fitxer (per depurar):\n",
      "   ['id,label\\n', '1,D0X\\n', '2,B0A\\n']\n",
      "üìã Classes carregades (14): ['D0X', 'B0A', 'B0B', 'G01', 'G02', 'G03', 'G04', 'G05', 'G06', 'G07', 'G08', 'G09', 'G10', 'G11']\n",
      "‚ÑπÔ∏è L'ID m√≠nim √©s 1.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. LLEGIR CLASSES (Versi√≥ \"Blindada\") ---\n",
    "def carregar_classes_txt(path):\n",
    "    print(f\"Llegint classes de: {path}\")\n",
    "    classes = {}\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"ERROR: El fitxer {path} no existeix.\")\n",
    "        return [\"NoGesture\"], 1\n",
    "\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        print(f\" Primeres l√≠nies del fitxer (per depurar):\\n   {lines[:3]}\")\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            \n",
    "            # 1. Intentem separar per diferents car√†cters\n",
    "            parts = []\n",
    "            if ',' in line:\n",
    "                parts = line.split(',') # Format CSV: 1,NoGesture\n",
    "            elif ';' in line:\n",
    "                parts = line.split(';') # Format CSV rar\n",
    "            else:\n",
    "                parts = line.split()    # Format espais: 1 NoGesture\n",
    "            \n",
    "            # 2. Si tenim dades suficients\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    # Netegem l'ID (per si t√© espais o car√†cters invisibles)\n",
    "                    id_str = \"\".join(filter(str.isdigit, parts[0]))\n",
    "                    if not id_str: continue \n",
    "                    \n",
    "                    id_class = int(id_str)\n",
    "                    \n",
    "                    # El nom √©s la segona part (netejant cometes o espais)\n",
    "                    name_class = parts[1].strip().strip('\"').strip(\"'\")\n",
    "                    \n",
    "                    classes[id_class] = name_class\n",
    "                except ValueError:\n",
    "                    # Si la primera columna no √©s un n√∫mero (ex: cap√ßalera \"ID Class\")\n",
    "                    continue\n",
    "        \n",
    "        # 3. Validaci√≥ final\n",
    "        if not classes:\n",
    "            print(\"ALERTA: No s'ha pogut llegir cap classe. El fitxer est√† buit o t√© un format desconegut.\")\n",
    "            return [\"NoGesture\"], 1\n",
    "            \n",
    "        # 4. Ordenem\n",
    "        sorted_ids = sorted(classes.keys())\n",
    "        class_list = [classes[i] for i in sorted_ids]\n",
    "        min_id = sorted_ids[0]\n",
    "        \n",
    "        return class_list, min_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error cr√≠tic (Excepci√≥): {e}\")\n",
    "        return [\"NoGesture\"], 1\n",
    "\n",
    "CLASSES, MIN_ID = carregar_classes_txt(FILE_CLASS_IDX)\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"Classes carregades ({NUM_CLASSES}): {CLASSES}\")\n",
    "print(f\"L'ID m√≠nim √©s {MIN_ID}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. MODEL EUREKA ---\n",
    "class EurekaNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EurekaNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(INPUT_SIZE, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.fc3 = nn.Linear(64, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. EXTRACTOR DE FEATURES (Optimitzat) ---\n",
    "def extract_features(video_path, start, end, hands_model):\n",
    "    \"\"\"\n",
    "    Ara rep el model de mans ja carregat per no perdre temps reiniciant-lo.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # Validaci√≥ r√†pida: si no pot obrir el v√≠deo, sortim\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    \n",
    "    lm_seq = []\n",
    "    frames_to_read = end - start + 1\n",
    "    if frames_to_read < M_KEY_FRAMES: frames_to_read = M_KEY_FRAMES\n",
    "    \n",
    "    for _ in range(frames_to_read):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        # Convertim a RGB per a MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Usem el model passat per par√†metre (Molt m√©s r√†pid!)\n",
    "        res = hands_model.process(frame_rgb)\n",
    "        \n",
    "        if res.multi_hand_landmarks:\n",
    "            lm_seq.append([[lm.x, lm.y] for lm in res.multi_hand_landmarks[0].landmark])\n",
    "    \n",
    "    cap.release()\n",
    "    # NO TANQUEM EL MODEL AQU√ç (hands.close()) PERQU√à EL REUTILITZAREM\n",
    "    \n",
    "    if len(lm_seq) < 2: return None\n",
    "    while len(lm_seq) < M_KEY_FRAMES: lm_seq.append(lm_seq[-1])\n",
    "    \n",
    "    indices = np.linspace(0, len(lm_seq)-1, M_KEY_FRAMES, dtype=int)\n",
    "    sel_frames = [lm_seq[i] for i in indices]\n",
    "    \n",
    "    feats = []\n",
    "    for t in range(1, len(sel_frames)):\n",
    "        curr, prev = sel_frames[t], sel_frames[t-1]\n",
    "        for pc in curr:\n",
    "            for pp in prev:\n",
    "                feats.extend([pc[0]-pp[0], pc[1]-pp[1]])\n",
    "    return np.array(feats, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. INDEXADOR DE V√çDEOS ---\n",
    "def indexar_videos_local(root_dir):\n",
    "    print(f\"üîç Indexant v√≠deos a: {os.path.abspath(root_dir)}\")\n",
    "    video_map = {}\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.avi', '.mp4')):\n",
    "                name_no_ext = os.path.splitext(file)[0]\n",
    "                full_path = os.path.join(root, file)\n",
    "                video_map[name_no_ext] = full_path\n",
    "                count += 1\n",
    "    print(f\"Trobats {count} fitxers de v√≠deo.\")\n",
    "    return video_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. PREPARAR DATASET (Optimitzat amb c√†rrega √∫nica) ---\n",
    "def preparar_dataset_txt():\n",
    "    # 1. Indexar disc\n",
    "    video_map = indexar_videos_local(PATH_VIDEOS_ROOT)\n",
    "    if not video_map:\n",
    "        print(\"ERROR: Carpeta de v√≠deos buida.\")\n",
    "        return [], []\n",
    "\n",
    "    # 2. Llegir Annot_TrainList.txt\n",
    "    print(f\"Llegint llista d'entrenament: {FILE_TRAIN_LIST}\")\n",
    "    try:\n",
    "        df = pd.read_csv(FILE_TRAIN_LIST, header=None, sep=',')\n",
    "        df.columns = ['video', 'label_name', 'label_id', 'start', 'end', 'frames']\n",
    "    except Exception as e:\n",
    "        print(f\"Error llegint el CSV: {e}\")\n",
    "        return [], []\n",
    "\n",
    "    X, Y = [], []\n",
    "    stats = {\"ok\": 0, \"fail_read\": 0, \"no_video\": 0}\n",
    "    \n",
    "    print(\"Inicialitzant MediaPipe (una sola vegada)...\")\n",
    "    mp_hands = mp.solutions.hands\n",
    "    \n",
    "    # INICIALITZEM EL \"MOTOR\" AQU√ç FORA DEL BUCLE\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=False, # False √©s m√©s r√†pid per a v√≠deo\n",
    "        max_num_hands=1, \n",
    "        min_detection_confidence=0.4\n",
    "    ) as hands_model:\n",
    "        \n",
    "        print(f\"Processant {len(df)} gestos (Ara anir√† r√†pid)...\")\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            try:\n",
    "                vid_name = str(row['video']).strip()\n",
    "                label_id = int(row['label_id']) - MIN_ID\n",
    "                start = int(row['start'])\n",
    "                end = int(row['end'])\n",
    "                \n",
    "                if label_id < 0 or label_id >= NUM_CLASSES: continue\n",
    "\n",
    "                # Buscar v√≠deo\n",
    "                vpath = video_map.get(vid_name)\n",
    "                if not vpath:\n",
    "                    clean = os.path.splitext(vid_name)[0]\n",
    "                    vpath = video_map.get(clean)\n",
    "                    \n",
    "                if not vpath:\n",
    "                    stats[\"no_video\"] += 1\n",
    "                    continue\n",
    "                    \n",
    "                # PASSEM EL MODEL (hands_model) A LA FUNCI√ì\n",
    "                feats = extract_features(vpath, start, end, hands_model)\n",
    "                \n",
    "                if feats is not None and len(feats) == INPUT_SIZE:\n",
    "                    X.append(torch.tensor(feats))\n",
    "                    Y.append(label_id)\n",
    "                    stats[\"ok\"] += 1\n",
    "                else:\n",
    "                    stats[\"fail_read\"] += 1\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    print(f\"\\nRESUM FINAL:\")\n",
    "    print(f\"   Processats OK: {stats['ok']}\")\n",
    "    print(f\"   Fallats (MediaPipe): {stats['fail_read']}\")\n",
    "    print(f\"   V√≠deo no trobat: {stats['no_video']}\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. ENTRENAMENT ---\n",
    "class HandDataset(Dataset):\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]\n",
    "\n",
    "def train_local():\n",
    "    X, Y = preparar_dataset_txt()\n",
    "    \n",
    "    if not X:\n",
    "        print(\"No hi ha dades per entrenar.\")\n",
    "        return\n",
    "\n",
    "    dataset = HandDataset(torch.stack(X), torch.tensor(Y, dtype=torch.long))\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    print(f\"Iniciant entrenament al dispositiu: {DEVICE}\")\n",
    "    \n",
    "    model = EurekaNet().to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    EPOCHS = 100\n",
    "    for ep in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tot_loss += loss.item()\n",
    "            _, pred = torch.max(out, 1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            \n",
    "        print(f\"Epoch {ep+1}/{EPOCHS}: Acc {100*correct/total:.2f}% | Loss {tot_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"eureka_model_100_epochs.pth\")\n",
    "    print(f\"Model guardat a: {os.path.abspath('eureka_model_100_epochs.pth')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Indexant v√≠deos a: /Users/jaumemil/Desktop/IA/TFG/Eureka/videos\n",
      "‚úÖ Trobats 200 fitxers de v√≠deo.\n",
      "üìñ Llegint llista d'entrenament: annotations/Annot_TrainList.txt\n",
      "‚öôÔ∏è Inicialitzant MediaPipe (una sola vegada)...\n",
      "üöÄ Processant 4039 gestos (Ara anir√† r√†pid)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765796948.343319 1305568 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280ecc0fc20b44b893148ee5b0e90560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1765796948.370965 1645173 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765796948.382114 1645173 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä RESUM FINAL:\n",
      "   ‚úÖ Processats OK: 3956\n",
      "   ‚ùå Fallats (MediaPipe): 83\n",
      "   ‚ùå V√≠deo no trobat: 0\n",
      "üöÄ Iniciant entrenament al dispositiu: mps\n",
      "Epoch 1/100: Acc 49.70% | Loss 1.6651\n",
      "Epoch 2/100: Acc 69.44% | Loss 1.0356\n",
      "Epoch 3/100: Acc 75.35% | Loss 0.8172\n",
      "Epoch 4/100: Acc 78.08% | Loss 0.7100\n",
      "Epoch 5/100: Acc 77.93% | Loss 0.6722\n",
      "Epoch 6/100: Acc 79.50% | Loss 0.6111\n",
      "Epoch 7/100: Acc 80.86% | Loss 0.5742\n",
      "Epoch 8/100: Acc 81.12% | Loss 0.5624\n",
      "Epoch 9/100: Acc 81.72% | Loss 0.5386\n",
      "Epoch 10/100: Acc 81.67% | Loss 0.5181\n",
      "Epoch 11/100: Acc 83.14% | Loss 0.5021\n",
      "Epoch 12/100: Acc 83.77% | Loss 0.4692\n",
      "Epoch 13/100: Acc 83.97% | Loss 0.4750\n",
      "Epoch 14/100: Acc 84.10% | Loss 0.4443\n",
      "Epoch 15/100: Acc 84.48% | Loss 0.4473\n",
      "Epoch 16/100: Acc 85.59% | Loss 0.4229\n",
      "Epoch 17/100: Acc 85.19% | Loss 0.4246\n",
      "Epoch 18/100: Acc 85.26% | Loss 0.4138\n",
      "Epoch 19/100: Acc 84.98% | Loss 0.4158\n",
      "Epoch 20/100: Acc 86.27% | Loss 0.3938\n",
      "Epoch 21/100: Acc 86.50% | Loss 0.3887\n",
      "Epoch 22/100: Acc 86.02% | Loss 0.3726\n",
      "Epoch 23/100: Acc 86.63% | Loss 0.3772\n",
      "Epoch 24/100: Acc 86.15% | Loss 0.3696\n",
      "Epoch 25/100: Acc 87.59% | Loss 0.3539\n",
      "Epoch 26/100: Acc 87.23% | Loss 0.3660\n",
      "Epoch 27/100: Acc 87.36% | Loss 0.3464\n",
      "Epoch 28/100: Acc 87.61% | Loss 0.3500\n",
      "Epoch 29/100: Acc 88.20% | Loss 0.3300\n",
      "Epoch 30/100: Acc 88.42% | Loss 0.3255\n",
      "Epoch 31/100: Acc 88.17% | Loss 0.3293\n",
      "Epoch 32/100: Acc 88.25% | Loss 0.3235\n",
      "Epoch 33/100: Acc 88.47% | Loss 0.3200\n",
      "Epoch 34/100: Acc 88.85% | Loss 0.3098\n",
      "Epoch 35/100: Acc 88.80% | Loss 0.3110\n",
      "Epoch 36/100: Acc 88.62% | Loss 0.3231\n",
      "Epoch 37/100: Acc 89.28% | Loss 0.3000\n",
      "Epoch 38/100: Acc 89.61% | Loss 0.2871\n",
      "Epoch 39/100: Acc 88.83% | Loss 0.3002\n",
      "Epoch 40/100: Acc 88.80% | Loss 0.3057\n",
      "Epoch 41/100: Acc 89.31% | Loss 0.2967\n",
      "Epoch 42/100: Acc 90.47% | Loss 0.2674\n",
      "Epoch 43/100: Acc 89.89% | Loss 0.2817\n",
      "Epoch 44/100: Acc 89.94% | Loss 0.2801\n",
      "Epoch 45/100: Acc 89.74% | Loss 0.2814\n",
      "Epoch 46/100: Acc 89.56% | Loss 0.2810\n",
      "Epoch 47/100: Acc 90.12% | Loss 0.2830\n",
      "Epoch 48/100: Acc 90.82% | Loss 0.2497\n",
      "Epoch 49/100: Acc 90.34% | Loss 0.2737\n",
      "Epoch 50/100: Acc 91.00% | Loss 0.2581\n",
      "Epoch 51/100: Acc 90.27% | Loss 0.2663\n",
      "Epoch 52/100: Acc 91.03% | Loss 0.2583\n",
      "Epoch 53/100: Acc 90.82% | Loss 0.2604\n",
      "Epoch 54/100: Acc 91.10% | Loss 0.2488\n",
      "Epoch 55/100: Acc 91.00% | Loss 0.2531\n",
      "Epoch 56/100: Acc 91.53% | Loss 0.2351\n",
      "Epoch 57/100: Acc 91.13% | Loss 0.2477\n",
      "Epoch 58/100: Acc 91.43% | Loss 0.2448\n",
      "Epoch 59/100: Acc 91.28% | Loss 0.2555\n",
      "Epoch 60/100: Acc 91.58% | Loss 0.2369\n",
      "Epoch 61/100: Acc 91.78% | Loss 0.2309\n",
      "Epoch 62/100: Acc 91.10% | Loss 0.2506\n",
      "Epoch 63/100: Acc 91.86% | Loss 0.2405\n",
      "Epoch 64/100: Acc 91.71% | Loss 0.2274\n",
      "Epoch 65/100: Acc 91.61% | Loss 0.2433\n",
      "Epoch 66/100: Acc 92.59% | Loss 0.2108\n",
      "Epoch 67/100: Acc 92.04% | Loss 0.2309\n",
      "Epoch 68/100: Acc 91.73% | Loss 0.2390\n",
      "Epoch 69/100: Acc 92.24% | Loss 0.2205\n",
      "Epoch 70/100: Acc 92.42% | Loss 0.2160\n",
      "Epoch 71/100: Acc 92.29% | Loss 0.2242\n",
      "Epoch 72/100: Acc 92.34% | Loss 0.2103\n",
      "Epoch 73/100: Acc 92.75% | Loss 0.2168\n",
      "Epoch 74/100: Acc 92.80% | Loss 0.2140\n",
      "Epoch 75/100: Acc 92.44% | Loss 0.2311\n",
      "Epoch 76/100: Acc 91.86% | Loss 0.2161\n",
      "Epoch 77/100: Acc 92.92% | Loss 0.1918\n",
      "Epoch 78/100: Acc 92.26% | Loss 0.2199\n",
      "Epoch 79/100: Acc 92.34% | Loss 0.2092\n",
      "Epoch 80/100: Acc 92.49% | Loss 0.2060\n",
      "Epoch 81/100: Acc 92.67% | Loss 0.1969\n",
      "Epoch 82/100: Acc 92.75% | Loss 0.2131\n",
      "Epoch 83/100: Acc 92.69% | Loss 0.2082\n",
      "Epoch 84/100: Acc 92.34% | Loss 0.2181\n",
      "Epoch 85/100: Acc 93.25% | Loss 0.2048\n",
      "Epoch 86/100: Acc 93.35% | Loss 0.1936\n",
      "Epoch 87/100: Acc 92.85% | Loss 0.2049\n",
      "Epoch 88/100: Acc 92.95% | Loss 0.1979\n",
      "Epoch 89/100: Acc 93.00% | Loss 0.2030\n",
      "Epoch 90/100: Acc 92.47% | Loss 0.2116\n",
      "Epoch 91/100: Acc 93.12% | Loss 0.2053\n",
      "Epoch 92/100: Acc 93.53% | Loss 0.1853\n",
      "Epoch 93/100: Acc 92.95% | Loss 0.1993\n",
      "Epoch 94/100: Acc 93.23% | Loss 0.1933\n",
      "Epoch 95/100: Acc 92.82% | Loss 0.2137\n",
      "Epoch 96/100: Acc 93.58% | Loss 0.1906\n",
      "Epoch 97/100: Acc 93.86% | Loss 0.1775\n",
      "Epoch 98/100: Acc 93.66% | Loss 0.1808\n",
      "Epoch 99/100: Acc 93.68% | Loss 0.1775\n",
      "Epoch 100/100: Acc 93.83% | Loss 0.1774\n",
      "‚úÖ Model guardat a: /Users/jaumemil/Desktop/IA/TFG/Eureka/eureka_model_100_epochs.pth\n"
     ]
    }
   ],
   "source": [
    "# --- 9. EXECUTAR ---\n",
    "if __name__ == \"__main__\":\n",
    "    train_local()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
